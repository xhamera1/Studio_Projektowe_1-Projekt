{
 "cells": [
  {
   "cell_type": "code",
   "id": "2ca2eb17-5c89-4b02-aa19-333a40ea1fd6",
   "metadata": {},
   "source": [
    "# https://archive.ics.uci.edu/dataset/45/heart+disease"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ],
   "id": "8fd64473805d1523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = pd.read_csv(\"../data/heart_attack_prediction_dataset.csv\")",
   "id": "73902d691c2381f2"
  },
  {
   "cell_type": "code",
   "id": "228afbe6-f9a1-49bf-951f-2ec42bea59ac",
   "metadata": {},
   "source": [
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"First 15 rows:\")\n",
    "display(df.head(15))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b053fefd-dbc5-4dac-86df-e96e1c12fa8d",
   "metadata": {},
   "source": [
    "print(df.info())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24de932d-e01f-4f3e-a3aa-6e03a0c60ce9",
   "metadata": {},
   "source": [
    "print(df.isnull().sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data Preprocessing & Feature Engineering\n",
    "\n",
    "# Count missing values in the problematic features\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'feature': ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca',\n",
    "                'thal'],\n",
    "    'total_missing': [\n",
    "        (df['age'] == '?').sum(),\n",
    "        (df['sex'] == '?').sum(),\n",
    "        (df['cp'] == '?').sum(),\n",
    "        (df['trestbps'] == '?').sum(),\n",
    "        (df['chol'] == '?').sum(),\n",
    "        (df['fbs'] == '?').sum(),\n",
    "        (df['restecg'] == '?').sum(),\n",
    "        (df['thalach'] == '?').sum(),\n",
    "        (df['exang'] == '?').sum(),\n",
    "        (df['oldpeak'] == '?').sum(),\n",
    "        (df['slope'] == '?').sum(),\n",
    "        (df['ca'] == '?').sum(),\n",
    "        (df['thal'] == '?').sum(),\n",
    "    ],\n",
    "    'missing_percentage': [\n",
    "        ((df['age'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['sex'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['cp'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['trestbps'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['chol'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['fbs'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['restecg'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['thalach'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['exang'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['oldpeak'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['slope'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['thal'] == '?').sum() / len(df)) * 100,\n",
    "        ((df['ca'] == '?').sum() / len(df)) * 100\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Missing Value Analysis:\")\n",
    "print(missing_analysis)"
   ],
   "id": "aca0a68a50cd8998"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "def clean_heart_disease_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive cleaning function for UCI Heart Disease dataset\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Step 1: Drop high-missing columns\n",
    "    columns_to_drop = ['slope', 'thal', 'ca']\n",
    "    df_clean = df_clean.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # Step 2: Clean column names\n",
    "    df_clean.columns = df_clean.columns.str.strip()\n",
    "\n",
    "    # Step 3: Replace '?' with NaN for proper handling\n",
    "    df_clean = df_clean.replace('?', np.nan)\n",
    "\n",
    "    # Step 4: Convert numerical columns to proper numeric type\n",
    "    numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "    for col in numerical_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "    # Step 5: Convert categorical columns (handle any remaining '?')\n",
    "    categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang']\n",
    "    for col in categorical_cols:\n",
    "        if col in df_clean.columns:\n",
    "            # Replace any non-numeric values with NaN, then convert to numeric\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "    # Step 6: Impute missing values using KNNImputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    df_clean = pd.DataFrame(imputer.fit_transform(df_clean), columns=df_clean.columns)\n",
    "\n",
    "    # Step 7: Drop rows with any remaining missing values\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.dropna()\n",
    "    final_rows = len(df_clean)\n",
    "\n",
    "    print(f\"  Data cleaning completed:\")\n",
    "    print(f\"  Rows removed due to missing values: {initial_rows - final_rows}\")\n",
    "    print(f\"  Final dataset shape: {df_clean.shape}\")\n",
    "\n",
    "    # Step 8: Create binary target\n",
    "    df_clean['heart_disease'] = (df_clean['num'] > 0).astype(int)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Apply the comprehensive cleaning\n",
    "df_clean = clean_heart_disease_data(df)\n",
    "\n",
    "# Verify no more '?' values\n",
    "print(\" Verification - No more '?' values:\")\n",
    "for col in df_clean.columns:\n",
    "    if (df_clean[col] == '?').any():\n",
    "        print(f\"  WARNING: Still found '?' in {col}\")\n",
    "    else:\n",
    "        print(f\"  {col}: Clean\")"
   ],
   "id": "43affcf6304c9d49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "315e3adf780ef920"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create binary target (using the clean 'num' column)\n",
    "df_clean['heart_disease'] = (df_clean['num'] > 0).astype(int)\n",
    "\n",
    "print(\"  Target Distribution:\")\n",
    "print(df_clean['heart_disease'].value_counts())\n",
    "print(f\"Baseline accuracy: {max(df_clean['heart_disease'].value_counts(normalize=True)):.3f}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_clean.drop(['num', 'heart_disease'], axis=1)\n",
    "y = df_clean['heart_disease']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\" Dataset Split:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the numerical features\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "print(\"  Feature scaling completed successfully!\")"
   ],
   "id": "444bbe68fa575143"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import all required models and utilities\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, \\\n",
    "    ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "ce4e8efc5f104625"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=13, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=13, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=13),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=13),\n",
    "    'SVM': SVC(kernel='linear', random_state=13, class_weight='balanced', probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=13, class_weight='balanced')\n",
    "}"
   ],
   "id": "c80fb07108d758cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For models that need scaling, create pipelines\n",
    "scaling_models = ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']\n",
    "model_pipelines = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name in scaling_models:\n",
    "        # Create pipeline with scaling for models that need it\n",
    "        model_pipelines[name] = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "    else:\n",
    "        # Tree-based models don't need scaling\n",
    "        model_pipelines[name] = model\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)\n",
    "scoring = make_scorer(accuracy_score)\n",
    "\n",
    "# Perform cross-validation for each model\n",
    "cv_results = {}\n",
    "print(\" Running 5-Fold Cross-Validation...\")\n",
    "for name, model in model_pipelines.items():\n",
    "    try:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train,\n",
    "                                    cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        cv_results[name] = {\n",
    "            'mean_accuracy': cv_scores.mean(),\n",
    "            'std_accuracy': cv_scores.std(),\n",
    "            'all_scores': cv_scores\n",
    "        }\n",
    "        print(f\" {name:25s}: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\" {name:25s}: Error - {str(e)}\")\n",
    "        cv_results[name] = {'mean_accuracy': 0, 'std_accuracy': 0, 'all_scores': []}\n",
    "\n",
    "# Display results in sorted order\n",
    "print(\" Cross-Validation Results (Sorted by Performance):\")\n",
    "print(\"=\" * 60)\n",
    "sorted_results = sorted(cv_results.items(), key=lambda x: x[1]['mean_accuracy'], reverse=True)\n",
    "\n",
    "for name, results in sorted_results:\n",
    "    print(f\"{name:25s}: {results['mean_accuracy']:.4f} ± {results['std_accuracy']:.4f}\")\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = sorted_results[0][0]\n",
    "best_cv_score = sorted_results[0][1]['mean_accuracy']\n",
    "print(f\" Best Model: {best_model_name} (CV Accuracy: {best_cv_score:.4f})\")"
   ],
   "id": "be4632514799a132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create visualization of model performance\n",
    "plt.figure(figsize=(14, 10))\n",
    "model_names = [name for name, _ in sorted_results]\n",
    "mean_scores = [results['mean_accuracy'] for _, results in sorted_results]\n",
    "std_scores = [results['std_accuracy'] for _, results in sorted_results]\n",
    "\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))\n",
    "bars = plt.barh(range(len(model_names)), mean_scores, xerr=std_scores,\n",
    "                alpha=0.8, color=colors, edgecolor='black', height=0.7)\n",
    "\n",
    "plt.yticks(range(len(model_names)), model_names)\n",
    "plt.xlabel('Accuracy Score', fontsize=12)\n",
    "plt.title('Model Performance Comparison(5-Fold Cross-Validation ± Standard Deviation)', fontsize=14, fontweight='bold')\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (mean, std) in enumerate(zip(mean_scores, std_scores)):\n",
    "    plt.text(mean + 0.01, i, f'{mean:.3f} ± {std:.3f}',\n",
    "             va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show individual fold scores for the best model\n",
    "print(f\" Detailed CV Scores for {best_model_name}:\")\n",
    "best_scores = cv_results[best_model_name]['all_scores']\n",
    "for fold, score in enumerate(best_scores, 1):\n",
    "    print(f\"  Fold {fold}: {score:.4f}\")"
   ],
   "id": "1d67bc5083c94b99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "adaBoost = AdaBoostClassifier(n_estimators=200, random_state=13)\n",
    "\n",
    "adaBoost.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = adaBoost.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Final Performance Comparison:\")\n",
    "print(f\"Cross-Validation Accuracy: {best_cv_score:.4f}\")\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Generalization Gap: {abs(best_cv_score - test_accuracy):.4f}\")\n",
    "\n",
    "if abs(best_cv_score - test_accuracy) < 0.05:\n",
    "    print(\"Good generalization: CV and test performance are close!\")\n",
    "else:\n",
    "    print(\"Potential overfitting: Large gap between CV and test performance\")\n",
    "\n",
    "print(\" Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Disease', 'Disease']))\n",
    "\n",
    "coef = adaBoost.feature_importances_\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'coefficient': coef,\n",
    "    'abs_importance': np.abs(coef)\n",
    "}).sort_values('abs_importance', ascending=False)\n",
    "\n",
    "print(f\"AdaBoost features importance:\\n {feature_importance}\")"
   ],
   "id": "5a062174ddf81948"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define low-importance features (based on feature_importances_)\n",
    "drop_features = ['restecg', 'fbs']\n",
    "\n",
    "# Drop those features from training and test sets\n",
    "X_train_reduced = X_train.drop(columns=drop_features)\n",
    "X_test_reduced = X_test.drop(columns=drop_features)\n",
    "\n",
    "# Initialize AdaBoost with tuned hyperparameters\n",
    "ada_reduced = AdaBoostClassifier(n_estimators=200, random_state=13)\n",
    "\n",
    "ada_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = ada_reduced.predict(X_test_reduced)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Final Performance Comparison:\")\n",
    "print(f\"Cross-Validation Accuracy: {best_cv_score:.4f}\")\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Generalization Gap: {abs(best_cv_score - test_accuracy):.4f}\")\n",
    "\n",
    "if abs(best_cv_score - test_accuracy) < 0.05:\n",
    "    print(\"Good generalization: CV and test performance are close!\")\n",
    "else:\n",
    "    print(\"Potential overfitting: Large gap between CV and test performance\")\n",
    "\n",
    "print(\" Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Disease', 'Disease']))\n",
    "\n",
    "coef = ada_reduced.feature_importances_\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_reduced.columns,\n",
    "    'coefficient': coef,\n",
    "    'abs_importance': np.abs(coef)\n",
    "}).sort_values('abs_importance', ascending=False)\n",
    "\n",
    "print(f\"AdaBoost features importance:\\n {feature_importance}\")"
   ],
   "id": "d4846b6095119a68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "y_proba = ada_reduced.predict_proba(X_test_reduced)[:, 1]\n",
    "\n",
    "# Compute ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = ada_reduced.predict(X_test_reduced)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve and PR-AUC\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f'PR-AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f0bc1edfca59aab5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model Saving\n",
    "import pickle\n",
    "\n",
    "model_path = \"../trained_models/heart_attack-ada_boost_model.pkl\"\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(ada_reduced, f)"
   ],
   "id": "ce8feda41810b724"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
